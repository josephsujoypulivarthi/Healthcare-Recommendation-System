{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c501e6c-d2c4-45a2-9f12-f9214b31b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    object \n",
      " 12  thal      303 non-null    object \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(11), int64(1), object(2)\n",
      "memory usage: 33.3+ KB\n",
      "None\n",
      "\n",
      "Sample data:\n",
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
      "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
      "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
      "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
      "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
      "\n",
      "   slope   ca thal  target  \n",
      "0    3.0  0.0  6.0       0  \n",
      "1    2.0  3.0  3.0       2  \n",
      "2    2.0  2.0  7.0       1  \n",
      "3    3.0  0.0  3.0       0  \n",
      "4    1.0  0.0  3.0       0  \n",
      "\n",
      "Missing values before imputation:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          4\n",
      "thal        2\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Processed data shape: (303, 14)\n",
      "        age  sex   cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
      "0  0.948726  1.0  1.0  0.757525 -0.264900  1.0      2.0  0.017197    0.0   \n",
      "1  1.392002  1.0  4.0  1.611220  0.760415  0.0      2.0 -1.821905    1.0   \n",
      "2  1.392002  1.0  4.0 -0.665300 -0.342283  0.0      2.0 -0.902354    1.0   \n",
      "3 -1.932564  1.0  3.0 -0.096170  0.063974  0.0      0.0  1.637359    0.0   \n",
      "4 -1.489288  0.0  2.0 -0.096170 -0.825922  0.0      2.0  0.980537    0.0   \n",
      "\n",
      "    oldpeak  slope        ca  thal  target  \n",
      "0  1.087338    3.0 -0.711131   6.0       0  \n",
      "1  0.397182    2.0  2.504881   3.0       2  \n",
      "2  1.346147    2.0  1.432877   7.0       1  \n",
      "3  2.122573    3.0 -0.711131   3.0       0  \n",
      "4  0.310912    1.0 -0.711131   3.0       0  \n",
      "\n",
      "Training set: 242 samples\n",
      "Testing set: 61 samples\n",
      "\n",
      "Data preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "           'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "data = pd.read_csv('data/heart_disease.csv', names=columns)\n",
    "\n",
    "# Display initial data info\n",
    "print(\"Initial data info:\")\n",
    "print(data.info())\n",
    "print(\"\\nSample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Replace '?' with NaN\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "# Convert all columns to appropriate data types\n",
    "# First, handle numeric columns\n",
    "numeric_columns = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "for col in numeric_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Convert categorical columns to numeric as well\n",
    "categorical_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal', 'target']\n",
    "for col in categorical_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Now handle missing values column by column\n",
    "print(\"\\nMissing values before imputation:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# For numeric columns, use median imputation\n",
    "for col in numeric_columns:\n",
    "    median_value = data[col].median()\n",
    "    data[col] = data[col].fillna(median_value)\n",
    "\n",
    "# For categorical columns, use mode imputation\n",
    "for col in categorical_columns:\n",
    "    mode_value = data[col].mode()[0]  # Get the most frequent value\n",
    "    data[col] = data[col].fillna(mode_value)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Save processed data\n",
    "data.to_csv('data/processed_heart_data.csv', index=False)\n",
    "\n",
    "print(\"\\nProcessed data shape:\", data.shape)\n",
    "print(data.head())\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save splits for later use\n",
    "import pickle\n",
    "with open('data/train_test_split.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), f)\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
